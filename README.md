# HACKATHON_LTD_-UniGoTo-
[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://unigoto.streamlit.app/)

<img src="https://github.com/Zagalskiy/ml_web_app/blob/main/Las_Teteras_Desesperadas.jpg" width="200" height="200" alt="Las Teteras Desesperadas">

# Документация к проекту на разработку рекомендательной системы
# для проекта UniGoTo. 

# 1. Введение

Цель проекта разработать ML-решение рекомендательной системы, способное сформировать персональные ранжированные списки направлений обучения в вузах для каждого уникального запроса на основе данных, вводимых пользователем рекомендательной системы.

Используются данные с информацией об интересах пользователя, включая фильмы, музыку, книги, игры, хобби и т. д. Данные представлены в виде отдельных страниц, массив в формате json из 999 значений. 

# 2. Архитектура рекомендательной системы:

- Парсинг данных с API, проверка столбцов, загрузка данных, формирование датасета.
- Фильтрация вспомогательных таблиц, очистка интересов от шумов, предобработка интересов.
- Модель рекомендации по интересам.

# 3. Описание рекомендательной системы:

Рекомендательная система использует разреженную матрицу TF-IDF для преобразования строк интересов пользователей и алгоритм косинусного сходства для вычисления схожести их интересов.

## parser.py
Описание: Скрипт выполняет запрос к API сервису, первичную обработку полученных данных в формате JSON и их сохранение в CSV-файлы с последующим архивированием. Он использует библиотеки os, requests,  json, time,  pandas и zipfile.

Входные данные:
- токен для доступа к API сервису.

Выходные данные: 
- ZIP-архив с исходными данными в формате CSV.

Шаги:
1. Задается токен для доступа к API сервису в переменной "token".
2. Задается начальная ("start_page ") и конечная ("end_page") страница запросов. Задается размер батча.
3. Задается ограничение на количество запросов в минуту в переменной "requests_limit".
4. Задаем путь к папке "raw_data/", которая будет использоваться для сохранения полученных данных. Если папка не существует, она будет создана.
5. Проходим по каждому батчу страниц и получаем JSON-данные. Для каждой страницы выполняется запрос к API сервису, указывая номер страницы и токен доступа. Если запрос успешен (код ответа 200), извлекаем JSON-данные и выполняем дополнительную обработку.
6. В полученных JSON-данных ищем информацию о стране и городе. Если информация найдена, добавляем ее в словарь с данными пользователя и удаляем соответствующие ключи "country" и "city". Если информация отсутствует, выводим сообщение об ошибке.
7. Создается DataFrame для хранения данных текущего батча. Добавляем данные пользователей в DataFrame.
8. Сохраняются данные текущего батча в отдельный файл CSV. Формируем уникальное имя файла на основе диапазона страниц текущего батча. Если данные не пусты, сохраняем DataFrame в файл CSV в кодировке UTF-8.
9. Сщздаются ZIP-архив и добавляем в него сохраненный CSV-файл. Если данные отсутствуют, выводим сообщение о том, что нет данных для сохранения в файле. После архивирования удаляем исходный CSV-файл.

## int_checker.py
Описание: Скрипт осуществляет проверку загруженных файлов. Проверке подлежат столбцы с типом данных integer. При нахождении строк с другими форматами происходит их удаление с последующим перезаписыванием файла. В коде использованы библиотеки  os, zipfile, shutil и pandas.

Входные данные: 
- ZIP-архив с исходными данными в формате CSV.
Выходные данные:
- ZIP-архив с исправленными данными в формате CSV.

Шаги:
1. Определяется директория скрипта и путь к папке проекта, путь к папке с исходными данными и папке для разархивированных данных. Получает список файлов в папке с исходными данными.
2. Задается список столбцов с целочисленными значениями.
3. Создается множество для записи файлов с ошибками и словарь для записи строк, которые нужно удалить из файлов.
4. Для каждого файла в списке проводится проверка, является ли файл ZIP-архивом, получаем полный путь к файлу и открываем. 
5. Проверяем первый CSV файл. Если в проверяемых столбцах находим значения отличные от типа integer, проверяем, существует ли уже данное значение в словаре rows_to_remove, добавляем название файла с ошибкой во множество файлов с ошибками и добавляем значения для удаления из файла. Проходим по каждому файлу внутри ZIP-архива.
6. Создается папка для разархивированных данных, если ее нет. Извлекаем содержимое ZIP-архивов и заменяем файлы с ошибками, удаляя строки с ошибками и перезаписывая файлы. Папку с разархивированными данными удаляем.

## loader.py
Описание: Скрипт осуществляет итоговую очистку и объединение данных в единый датафрейм, а также создает вспомогательные таблицы – справочники. В коде использованы библиотеки  re, os, zipfile и pandas.

Входные данные: 
- ZIP-архив с файлами в формате CSV.
Выходные данные:
- "combined_data.csv", "cities.csv", "countries.csv", "faculties.csv", "universities.csv" - файлы CSV с данными.
  
Шаги:
1.	Определяются переменные, пути к родительской папке проекта, к папке с файлами ZIP и папке для сохранения данных.
2.	Создается список для хранения DataFrame.
3.	Создается список столбцов для считывания.
4.	Считываются файлы, задаются пороговые значения и удаляем строки, где значения превышают пороговые.
6.	Создается паттерн для поиска русских символов. Фильтруем данные, оставляя только строки с русскими названиями ВУЗов и незаполненными значениями.
7.	Сохраняются вспомогательные таблицы в одноименные файлы CSV.
8.	Удаляются вспомогательные столбцы из исходного DataFrame и сохраняем его в файл CSV).

## aux_filter.py
Описание: Этот скрипт выполняет очистку данных из файлов вспомогательных таблиц и сохраняет результаты в одноименные файлы. Он использует библиотеки pandas, os и zipfile.

Входные данные:
- "universities.csv", "faculties.csv", "countries.csv", "cities.csv" - файлы CSV с данными, требующими очистки.
- "data" - папка, содержащая файлы данных.

Выходные данные:
- "universities_filtered.csv", "faculties_filtered.csv", "countries_filtered.csv", "cities_filtered.csv" - файлы CSV с очищенными данными.

Шаги:
1. Получает путь к текущей папке скриптов.
2. Создается список имён файлов для загрузки.
3. Создается список имён столбцов для фильтрации.
4. Применяется по каждому файлу в списке.
5. Проверяется наличие ZIP-архива и загружается файл CSV в DataFrame.
6. Удаляются дубликаты.
7. Сортируются по возрастанию первых столбцов, фильтруем названия ВУЗов.
8. Сохраняет отфильтрованные данные в файлы universities_filtered.csv", "faculties_filtered.csv", "countries_filtered.csv", "cities_filtered.csv".

## pre_filter.py
Описание: Этот скрипт выполняет очистку данных из файла "combined_data.csv" и сохраняет результаты в файл "prefiltered_data.csv". Он использует библиотеки pandas, os и zipfile.

Входные данные:
- "combined_data.csv" - файл CSV с данными, требующими очистки.
- "data" - папка, содержащая файлы данных.

Выходные данные:
- "prefiltered_data.csv" - файл CSV с очищенными данными.

Шаги:
1. Проверяется наличие ZIP-архива "combined_data.zip". Если архив существует, его содержимое извлекается в папку "data".
2. Файл "combined_data.csv" загружается в объект DataFrame.
3. Создается список столбцов интересов пользователей и список столбцов с числовыми значениями.
4. Удаляются дубликаты и строки, в которых количество заполненных столбцов интересов меньше 3.
5. Создается список рекламных слов и фраз.
6. Создаем функцию проверки наличия рекламных слов в столбцах интересов и удаляем строки, содержащие рекламные слова в столбцах интересов.
7. Преобразуется типы данных столбцов 'country_id' и остальных столбцов с числовыми значениями в int16 и int32 соответственно.
8. Сохраняет отфильтрованные данные в файл "prefiltered_data.csv".


## noise_filter.py
Описание: Этот скрипт выполняет очистку данных из файла "prefiltered_data.csv" и сохраняет результаты в файл "filtered_data.csv". Он использует библиотеки pandas, re, os и zipfile.

Входные данные:
- "prefiltered_data.csv" - файл CSV с данными, требующими очистки.
- "data" - папка, содержащая файлы данных.

Выходные данные:
- "filtered_data.csv" - файл CSV с очищенными данными.

Шаги:
1. Проверяется наличие ZIP-архива "prefiltered_data.zip". Если архив существует, его содержимое извлекается в папку "data".
2. Файл "prefiltered_data.csv" загружается в объект DataFrame.
3. Создается список столбцов интересов пользователей.
4. Определяется функция "clean_text", которая применяется к столбцам интересов для очистки текста от лишних символов.
5. Заменяются пустые значения на "NaN", удаляются строки с "NaN" во всех 5 столбцах интересов.
6. Очищенные данные сохраняются в файл "filtered_data.csv".

## preprocessor.py
Описание: Этот скрипт выполняет предобработку данных из файла "filtered_data.csv" и сохраняет результаты в файл "preprocessed_data.csv". Он использует библиотеки pandas, re, os, zipfile, nltk, stopwords, wordnet и nltk.corpus.

Входные данные:
- "filtered_data.csv" - файл CSV с данными, требующими предобработки.
- "data" - папка, содержащая файлы данных.

Выходные данные:
- "preprocessed_data.csv" - файл CSV с предобработанными данными.

Шаги:
1. Проверяется наличие необходимых ресурсов NLTK перед их загрузкой.
2. Загружаются необходимые ресурсы для обработки текстовых данных из Natural Language Toolkit.
3. Проверяется наличие необходимых библиотек и ресурсов для работы скрипта.
4. Проверяется наличие ZIP-архива "filtered_data.zip". Если архив существует, его содержимое извлекается в папку "data".
5. Файл "filtered_data.csv" загружается в объект DataFrame.
6. Создается список столбцов интересов пользователей.
7. Определяется функция "preprocess_text", которая применяется к столбцам интересов для предобработки текста.
8. Определяется функция "clean_text", которая применяется к столбцам интересов для очистки текста от лишних символов.
9. Применяется функция "preprocess_text" к каждому набору интересов для каждой строки в DataFrame.
10. Применяется функция "clean_text" к каждому набору интересов для каждой строки в DataFrame.
11. Удаляются столбцы interest_col.
12. Сохраняются очищенные данные в файл "preprocessed_data.csv".

## recommendator.py
Скрипт выполняет обработку данных из сформированных ранее файлов и выводит рекомендацию ТОП-20 рекомендованных ВУЗов на основе предпочтений других пользователей из региона. Он использует библиотеки pandas, os, zipfile, scipy, sklearn. 

Входные данные:
- "universities_filtered.csv", "faculties_filtered.csv", "countries_filtered.csv", "cities_filtered.csv", "cities_regions.csv". Каждый из этих файлов содержит информацию об университетах, факультетах, странах, городах и регионах соответственно.
- данные вводимые пользователем

Выходные данные:
- ТОП-20 рекомендованных ВУЗов

Шаги:
1. Загрузка данных из файлов CSV в отдельные DataFrame universities_filtered.csv, faculties_filtered.csv, countries_filtered.csv, cities_filtered.csv, cities_regions.csv. 
2. Заполняются пропущенные значения в столбце 'preprocessed_interests'.
3. Создаются словари интересов пользователя.
4. Загружаем интересы пользователя, проверяем, обрабатывам и сохраняем в файл user_data.csv. 
5. Создание разреженной матрицы TF-IDF для преобразования строк интересов пользователей. Создается экземпляр TfidfVectorizer, который применяется к столбцу 'preprocessed_interests' DataFrame "data". Результатом является разреженная матрица TF-IDF, которая сохраняется в переменную tfidf_matrix.
6. Создание разреженной матрицы TF-IDF для интересов пользователя. Используется экземпляр TfidfVectorizer, который применяется к столбцу 'preprocessed_interests' DataFrame user_data. Результат сохраняется в переменную user_tfidf_matrix.
7. Вычисление косинусного сходства пользователя с другими пользователями. Результат сохраняется в переменную user_cosine_sim.
8. Получение ТОП-20 индексов пользователей с наибольшими косинусными сходствами. Результат сохраняется в переменную top_indices.
9. Создается функция вывода рекомендаций.
10. Вывод ТОП-20 ВУЗов на основе предпочтений пользователей с возможностью выбора города и региона. 

# 4. Применение рекомендательной системы

Рекомендательная система предназначена для анализа и поиска взаимосвязей между интересами пользователей уже поступивших или закончивших ВУЗы и пользователей абитуриентов. Данная система может предложить список ВУЗов согласно предпочтениям пользователей из выбранного региона.

# 5. Заключение

Рекомендательная система на данном этапе предусматривает только рекомендации ВУЗов в России. Главные перспективы  развития идеи это:
- расширение географии ВУЗов и подключение иностранных языков
- включение средних учебных заведений, школ, лицеев и т.д. 

# Локальный запуск приложения

src/recommendator.py

# Над созданием проекта работали:

* Загальский Игорь
* Санникова Юлия
* Майнгерт Владимир
* Аванесян Тачат
* Мухачев Иван
* Антропова Наталия
